---
title: "STAT 473 Project"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(skimr)
library(MASS)
library(caret)
library(class)
library(ROCR)
library(ISLR)
library(boot)
library(tree)
library(randomForest)
library(gbm)
library(tidyverse)
library(cutpointr)
library(car)
#library(InformationValue)
library(glmnet)
library(e1071)


```



## Exploratory data analysis
```{r}

df = read.csv("C:/Users/RJ Burson/Downloads/savant_data.csv")

df = df[,c(18,19,25,26,27,32:36,77:83)]

df$runner_on_1= as.factor(ifelse(is.na(df$on_1b),0,1))
df$runner_on_2= as.factor(ifelse(is.na(df$on_2b),0,1))
df$runner_on_3= as.factor(ifelse(is.na(df$on_3b),0,1))

df$stand = as.factor(ifelse(df$stand=="R",1,0))

df = df[,-c(2,6,7,8)]



df= mutate(df, fastball=ifelse(df$pitch_name %in% c("4-Seam Fastball"),1,0))

df=df[,-9]

skim(df)
dim(df)

```



## Splitting the data set

```{r}
n = nrow(df)
prop = .8
set.seed(123)
train_id = sample(1:n, size = n*prop, replace = FALSE)
test_id = (1:n)[-which(1:n %in% train_id)]
train_set = df[train_id, ]
test_set = df[test_id, ]

```



## Multicolinearity

```{r}
# Logistic model
m1 = glm(fastball~stand+balls+strikes+outs_when_up+inning+at_bat_number+pitch_number+home_score+away_score+runner_on_1+runner_on_2+runner_on_3, data = train_set)

summary(m1)

varImp(m1)
vif(m1)

##  multicolinearity is a problem

## Model without at_bat_number and pitch_number. Chose these two variables because pith number would be correlated with the number of balls and strikes in a count in an ab. At bat number would be correlated to innint because they both have to do with how far into the game the ab is taking place. 

m1 = glm(fastball~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3, data = train_set)

#Attempting to predict the test set
pred = predict(m1, test_set, type="response")
optimal = optimalCutoff(test_set$fastball, pred)[1]
pred = as.factor(ifelse(predict(m1, test_set, type="response")>optimal,1,0))
tb = table(pred = pred, truth = test_set$fastball)
tb
(tb[1,1] + tb[2,2])/sum(tb)

vif(m1)

## Model without inning and pitch_number


m1 = glm(fastball~stand+balls+strikes+outs_when_up+at_bat_number+home_score+away_score+runner_on_1+runner_on_2+runner_on_3, data = train_set)

#Attempting to predict the test set
pred = predict(m1, test_set, type="response")
optimal = optimalCutoff(test_set$fastball, pred)[1]
pred = as.factor(ifelse(predict(m1, test_set, type="response")>optimal,1,0))
tb = table(pred = pred, truth = test_set$fastball)
tb
(tb[1,1] + tb[2,2])/sum(tb)

vif(m1)


######### Use models without at_bat_number and pitch_number
```



## Initial logistic model with all variables
```{r}

# Logistic model
m1 = glm(fastball~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3, data = train_set)

summary(m1)

varImp(m1)
vif(m1)


#Attempting to predict the test set
pred = predict(m1, test_set, type="response")
optimal = optimalCutoff(test_set$fastball, pred)[1]
pred = as.factor(ifelse(predict(m1, test_set, type="response")>optimal,1,0))
tb = table(pred = pred, truth = test_set$fastball)
tb
(tb[1,1] + tb[2,2])/sum(tb)

#Plot ROC curve
glm_pred_class = predict(m1, test_set, type="response")
pred = prediction(glm_pred_class, test_set$fastball)
perf= performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)

#AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc



############# Reduced glm model ####################


m1_reduced= glm(fastball~stand+balls+strikes+inning+runner_on_1, data = train_set)
summary(m1_reduced)

varImp(m1_reduced)

#Attempting to predict the test set
pred = predict(m1_reduced, test_set, type="response")
optimal = optimalCutoff(test_set$fastball, pred)[1]
pred = as.factor(ifelse(predict(m1_reduced, test_set, type="response")>optimal,1,0))
tb = table(pred = pred, truth = test_set$fastball)
tb
(tb[1,1] + tb[2,2])/sum(tb)

#Plot ROC curve
glm_pred_class = predict(m1_reduced, test_set, type="response")
pred = prediction(glm_pred_class, test_set$fastball)
perf= performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)

#AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc

```




## Lasso regression to deal with multicollinearity and see which variables to keep. 

```{r}

xmat = model.matrix(fastball~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3, train_set)[,-1]


y = train_set$fastball
xmat = apply(xmat, 2, function (x) scale(x, center=FALSE))

mod.lasso = glmnet(xmat, y, alpha=1, family="binomial")

plot(mod.lasso, xvar = "lambda", label = TRUE)

#Predicting lasso model
set.seed(123)
cv.out = cv.glmnet(xmat, y, alpha=1, nfolds=10, family=binomial)
best.lambda = cv.out$lambda.min


pcoefs = predict(mod.lasso, s = best.lambda, type = "coefficients")
pcoefs

mod.lasso.best = glmnet(xmat, y, alpha=1, lambda = best.lambda)
yhat.lasso.best = predict(mod.lasso.best, newx = xmat, type = "response")#predict.glmnet


#### Create confusion matrix for test set

new_xmat = model.matrix(fastball~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3, test_set)[,-1]

y = test_set$fastball


pred =  predict(mod.lasso.best, newx = new_xmat, type = "response")
optimal = optimalCutoff(test_set$fastball, pred)[1]
pred = as.factor(ifelse(predict(mod.lasso.best, new_xmat, type="response")>optimal,1,0))
tb = table(pred = pred, truth = test_set$fastball)
tb
(tb[1,1] + tb[2,2])/sum(tb)

```



## Since the data set has many training observation use QDA model instead of LDA.

```{r}

train_qda_fit = qda(fastball~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3, data = train_set)

# Confusion matrix for test set
qda_pred_class = predict(train_qda_fit, test_set)$class
tb = table(predict_status = qda_pred_class,
true_status=test_set$fastball)
tb
(tb[1,1] + tb[2,2])/sum(tb)


# ROC plot for test set
qda_pred = predict(train_qda_fit, test_set)
qda_pred_post = qda_pred$posterior[,2]
pred = prediction(qda_pred_post, test_set$fastball)
perf = performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)


## AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc
```


## Classification tree

```{r}

mod.tree = tree(fastball ~ stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , data = train_set)
summary(mod.tree)


plot(mod.tree)
text(mod.tree, pretty = 0)

set.seed(123)
cv.out = cv.tree(mod.tree, K = 6)

plot(cv.out$size, cv.out$dev, type = "b")

prune.mod = prune.tree(mod.tree,
best = cv.out$size[which.min(cv.out$dev)])
plot(prune.mod)
text(prune.mod, pretty = 0)

#### See how it predicts test data
yhat.test = predict(prune.mod, newdata = test_set)
y.test = test_set$fastball
#mse for train set
mean((y.test-yhat.test)^2)



# Confusion matrix

pred = predict(prune.mod, newdata = test_set)
optimal = optimalCutoff(test_set$fastball, pred)[1]
yhat.test_class_tree = as.factor(ifelse(predict(prune.mod, test_set)>optimal,1,0))
 tb = table(pred = yhat.test_class_tree, true=test_set$fastball)
 tb
(tb[1,1] + tb[2,2])/sum(tb)

#Plot ROC curve
tree_pred_class = predict(prune.mod, test_set)
pred = prediction((tree_pred_class), test_set$fastball)
perf= performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)

#AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc

```


## Bagging
```{r}

p = ncol(train_set) - 1
set.seed(123)
bag_fit = randomForest(fastball~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , data = train_set, mtry = p, importance = TRUE)


pred =  predict(bag_fit, test_set, type = "class")
optimal = optimalCutoff(test_set$fastball, pred)[1]
yhat.test_bag = as.factor(ifelse(predict(bag_fit, test_set, type = "class")>optimal,1,0))
tb_bag = table(pred = yhat.test_bag,
true = test_set$fastball)
tb_bag
(tb_bag[1,1] + tb_bag[2,2])/sum(tb_bag)

#Plot ROC curve
yhat.test_bag = predict(bag_fit, test_set, type = "class")
pred = prediction(as.numeric(yhat.test_bag), test_set$fastball)
perf= performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)

#AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc

#Which variables are most important
importance(bag_fit, type=2)

varImpPlot(bag_fit, main = "Variable Importance (Bagging)")

```


## Random Forest
```{r}
p = ncol(train_set) - 1
set.seed(123)
rf_fit = randomForest(fastball ~ stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , data = train_set,
mtry = round(sqrt(p)), importance = TRUE)


pred =  predict(rf_fit, test_set, type = "class")
optimal = optimalCutoff(test_set$fastball, pred)[1]
yhat.test_rf = as.factor(ifelse(predict(rf_fit, test_set, type = "class")>optimal,1,0))
tb_rf = table(pred = yhat.test_rf,
true = test_set$fastball)
tb_rf
(tb_rf[1,1] + tb_rf[2,2])/sum(tb_rf)

#Plot ROC curve
yhat.test_rf = predict(rf_fit, test_set, type = "class")
pred = prediction(as.numeric(yhat.test_rf), test_set$fastball)
perf= performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)

#AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc


#Which variables are most important
importance(rf_fit, type=2)

varImpPlot(rf_fit, main = "Variable Importance (Random Forest)")
```


## Boosting


```{r}

set.seed(123)
boost_fit = gbm(fastball~ stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , train_set, n.trees = 100,
shrinkage = 0.1, interaction.depth = 1,
distribution = "bernoulli")


pred =  predict(boost_fit, test_set, type = "response")
optimal = optimalCutoff(test_set$fastball, pred)[1]
yhat.test_boost = as.factor(ifelse(predict(boost_fit, test_set, type = "response")>optimal,1,0))
tb_boost = table(pred = yhat.test_boost,
true = test_set$fastball)
tb_boost
(tb_boost[1,1] + tb_boost[2,2])/sum(tb_boost)


#Plot ROC curve
yhat.test_boost = predict(boost_fit, test_set, type = "response")
pred = prediction(as.numeric(yhat.test_boost), test_set$fastball)
perf= performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)

#AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc

#Which variables are most important

summary(boost_fit)


## Boosting performs slightly better than bagging and random forest.
```


## Tune boosted Model

```{r}

grid = expand.grid(
n.trees_vec = c(100, 200, 300, 400),
shrinkage_vec = c(0.2, 0.1, 0.06, 0.05, 0.04, .02, .01),
interaction.depth_vec = c(1, 2, 3),
miss_classification_rate = NA,
time = NA
)


### Commented this out because the rmd file wouldnt not knit. ######
#set.seed(1)
#for(i in 1:nrow(grid)){
#time = system.time({
#boost_fit = gbm(fastball~ ., train_set,
#n.trees = grid$n.trees_vec[i],
#shrinkage = grid$shrinkage_vec[i],
#interaction.depth = grid$interaction.depth_vec[i],
#distribution = "bernoulli", cv.folds=5)

#grid$miss_classification_rate[i] =
#boost_fit$cv.error[which.min(boost_fit$cv.error)]
#grid$time[i] = time[["elapsed"]]
#}
#)

#}

head(grid %>% arrange(miss_classification_rate))



## Tune Boosted Model

set.seed(123)
boost_fit = gbm(fastball~ stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , train_set, n.trees = 200,
shrinkage = 0.04, interaction.depth = 2,
distribution = "bernoulli")


pred =  predict(boost_fit, test_set, type = "response")
optimal = optimalCutoff(test_set$fastball, pred)[1]
yhat.test_boost = as.factor(ifelse(predict(boost_fit, test_set, type = "response")>optimal,1,0))
tb_boost = table(pred = yhat.test_boost,
true = test_set$fastball)
tb_boost
(tb_boost[1,1] + tb_boost[2,2])/sum(tb_boost)


#Plot ROC curve
yhat.test_boost = predict(boost_fit, test_set, type = "response")
pred = prediction(as.numeric(yhat.test_boost), test_set$fastball)
perf= performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)

#AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc
```



## SVM


```{r}
# Linear
set.seed(123)
tune_svm = tune(svm, fastball ~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , data = train_set, kernel = "linear",
ranges = list(cost = seq(.01, 10, length.out=10)))
summary(tune_svm)


svm_fit = svm(fastball ~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , data = train_set, kernel = "linear", cost = 2.23,
scale = FALSE)

summary(svm_fit)

#Predicting the test set
pred = predict(svm_fit, test_set, type="response")
optimal = optimalCutoff(test_set$fastball, pred)[1]
yhat.test_svm = as.factor(ifelse(predict(svm_fit, test_set, type = "response")>optimal,1,0))
tb_svm = table(pred = yhat.test_svm, truth = test_set$fastball)
tb_svm
(tb_boost[1,1] + tb_boost[2,2])/sum(tb_boost)

#Plot ROC curve
yhat.test_svm = predict(svm_fit, test_set, type = "response")
pred = prediction(as.numeric(yhat.test_svm), test_set$fastball)
perf= performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)

#AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc



# Radial
set.seed(123)
tune_svm = tune(svm, fastball ~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , data = train_set, kernel = "radial",
ranges = list(cost = seq(.01, 10, length.out=10)))
summary(tune_svm)


svm_fit = svm(fastball ~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , data = train_set, kernel = "radial", cost = 1.12,
scale = FALSE)

summary(svm_fit)

#Predicting the test set
pred = predict(svm_fit, test_set, type="response")
optimal = optimalCutoff(test_set$fastball, pred)[1]
yhat.test_svm = as.factor(ifelse(predict(svm_fit, test_set, type = "response")>optimal,1,0))
tb_svm = table(pred = yhat.test_svm, truth = test_set$fastball)
tb_svm
(tb_boost[1,1] + tb_boost[2,2])/sum(tb_boost)

#Plot ROC curve
yhat.test_svm = predict(svm_fit, test_set, type = "response")
pred = prediction(as.numeric(yhat.test_svm), test_set$fastball)
perf= performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)

#AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc




#Polynomial

set.seed(123)
tune_svm = tune(svm, fastball ~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , data = train_set, kernel = "polynomial",
ranges = list(cost = seq(.01, 10, length.out=10)))
summary(tune_svm)


svm_fit = svm(fastball ~stand+balls+strikes+outs_when_up+inning+home_score+away_score+runner_on_1+runner_on_2+runner_on_3 , data = train_set, kernel = "polynomial", cost = 1.12,
scale = FALSE)

summary(svm_fit)

#Predicting the test set
pred = predict(svm_fit, test_set, type="response")
optimal = optimalCutoff(test_set$fastball, pred)[1]
yhat.test_svm = as.factor(ifelse(predict(svm_fit, test_set, type = "response")>optimal,1,0))
tb_svm = table(pred = yhat.test_svm, truth = test_set$fastball)
tb_svm
(tb_svm[1,1] + tb_svm[2,2])/sum(tb_svm)

#Plot ROC curve
yhat.test_svm = predict(svm_fit, test_set, type = "response")
pred = prediction(as.numeric(yhat.test_svm), test_set$fastball)
perf= performance(pred, "tpr", "fpr")
plot(perf, main = "ROC Curve")
abline(0, 1, lty=3)

#AUC value
auc = as.numeric(performance(pred, "auc")@y.values)
auc
```
